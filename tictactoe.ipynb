{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titre : Introduction à l'algorithme Double Q-learning\n",
    "\n",
    "### Introduction :\n",
    "L'apprentissage par renforcement est une branche de l'intelligence artificielle qui étudie comment un agent peut apprendre à prendre des décisions pour maximiser sa récompense dans un environnement donné. Parmi les algorithmes populaires d'apprentissage par renforcement, on trouve le Q-learning, qui est largement utilisé pour apprendre des politiques optimales dans des environnements discrets.\n",
    "\n",
    "Définition de l'algorithme Double Q-learning :\n",
    "Le Double Q-learning est une extension de l'algorithme Q-learning classique. Il a été introduit pour remédier à un problème courant rencontré avec le Q-learning : la surestimation des valeurs Q. En effet, dans le Q-learning traditionnel, l'estimation des valeurs Q est effectuée en utilisant les mêmes valeurs pour sélectionner et évaluer les actions futures. Cela peut conduire à une surestimation des valeurs Q et, par conséquent, à une politique sous-optimale.\n",
    "\n",
    "### Fonctionnement de l'algorithme :\n",
    "L'idée principale derrière le Double Q-learning est de maintenir deux ensembles de valeurs Q distincts, généralement désignés par Q1 et Q2. Au lieu d'utiliser les mêmes valeurs pour sélectionner et évaluer les actions futures, l'agent utilise l'une des tables Q pour sélectionner l'action suivante et l'autre table Q pour évaluer cette action. Par conséquent, cela permet de réduire la surestimation des valeurs Q, car les actions sont évaluées indépendamment de la sélection.\n",
    "\n",
    "### Avantages de l'algorithme Double Q-learning :\n",
    "\n",
    "Réduit la surestimation des valeurs Q : En utilisant deux tables Q distinctes, l'algorithme Double Q-learning atténue le problème de surestimation des valeurs Q, ce qui peut conduire à une politique plus robuste et plus précise.\n",
    "Améliore la stabilité de l'apprentissage : En réduisant la surestimation des valeurs Q, l'algorithme Double Q-learning peut améliorer la stabilité de l'apprentissage, en évitant les oscillations et les comportements sous-optimaux.\n",
    "Conclusion :\n",
    "En résumé, l'algorithme Double Q-learning est une extension puissante et efficace de l'algorithme Q-learning classique. En utilisant deux tables Q distinctes pour sélectionner et évaluer les actions futures, il permet de réduire la surestimation des valeurs Q, conduisant ainsi à des politiques plus robustes et plus précises dans les environnements d'apprentissage par renforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titre: Introduction à l'algorithme Double Q-learning\n",
    "\n",
    "### Introduction\n",
    "Le Q-learning est un algorithme populaire dans le domaine de l'apprentissage par renforcement pour apprendre à prendre des décisions séquentielles dans un environnement. L'algorithme Q-learning est bien connu pour son efficacité et sa simplicité, mais il peut également présenter des biais d'estimation, en particulier dans des environnements où les récompenses peuvent être sujettes à des fluctuations importantes.\n",
    "\n",
    "### Principe de base du Q-learning\n",
    "Le Q-learning est basé sur l'idée de l'apprentissage de la fonction d'action-valeur (Q-value), qui estime la valeur d'une action dans un état donné. L'algorithme met à jour cette fonction en utilisant une règle de mise à jour basée sur la récompense obtenue en prenant une action et en se déplaçant vers un nouvel état.\n",
    "\n",
    "### Limitations du Q-learning classique\n",
    "Une limitation majeure du Q-learning classique est son propension à surestimer les valeurs Q, en particulier dans des environnements où les récompenses peuvent être bruyantes ou sujettes à des variations aléatoires. Cela peut conduire à des politiques sous-optimales ou instables.\n",
    "\n",
    "### Présentation du Double Q-learning\n",
    "Pour surmonter les biais de surestimation des valeurs Q, le Double Q-learning propose une approche alternative. L'idée centrale derrière le Double Q-learning est d'utiliser deux ensembles de valeurs Q distincts pour estimer la valeur d'une action dans un état donné.\n",
    "\n",
    "### Mécanisme de mise à jour\n",
    "Au lieu de mettre à jour une seule fonction Q, le Double Q-learning alterne entre les deux ensembles de valeurs Q lors de la mise à jour. L'algorithme utilise l'un des ensembles de valeurs Q pour sélectionner l'action suivante et l'autre ensemble de valeurs Q pour estimer la valeur de cette action.\n",
    "\n",
    "### Avantages du Double Q-learning\n",
    "L'utilisation de deux ensembles de valeurs Q permet d'atténuer les biais de surestimation des valeurs Q. En alternant entre les deux ensembles, l'algorithme réduit la probabilité de sélection d'actions surestimées, ce qui peut conduire à des politiques plus stables et robustes.\n",
    "\n",
    "## Conclusion\n",
    "En conclusion, l'algorithme Double Q-learning offre une solution efficace pour surmonter les biais de surestimation des valeurs Q dans le cadre du Q-learning classique. En utilisant deux ensembles de valeurs Q distincts, l'algorithme permet d'atténuer les effets des fluctuations de récompense et de produire des politiques d'action plus fiables et robustes dans divers environnements d'apprentissage par renforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titre: Découverte de l'environnement du jeu du Morpion\n",
    "## Introduction\n",
    "Le jeu du morpion, également connu sous le nom de Tic-Tac-Toe, est un jeu de société classique qui se joue sur une grille de 3x3 cases. Le jeu oppose deux joueurs, qui alternent pour placer leur marque (X ou O) dans une case vide de la grille. Le premier joueur à aligner trois de ses marques horizontalement, verticalement ou en diagonale remporte la partie.\n",
    "\n",
    "## Description de l'environnement\n",
    "L'environnement du jeu du morpion est une représentation informatique du jeu, permettant à des agents d'apprentissage ou à des joueurs humains de jouer au morpion de manière virtuelle. La grille de jeu est généralement représentée sous forme de tableau à deux dimensions, où chaque case peut être vide, contenant la marque du joueur X ou la marque du joueur O.\n",
    "\n",
    "## Caractéristiques de l'environnement\n",
    "Grille de jeu : La grille de jeu est une matrice de 3x3 cases, où chaque case peut être vide, contenant la marque X ou la marque O.\n",
    "Actions : Les actions disponibles pour chaque joueur consistent à sélectionner une case vide de la grille où placer sa marque.\n",
    "Observations : L'état de l'environnement est représenté par la disposition actuelle des marques sur la grille.\n",
    "Récompenses : La récompense est généralement attribuée à la fin de la partie, lorsque l'un des joueurs a aligné trois de ses marques ou que la grille est pleine sans alignement.\n",
    "Terminalité : La partie se termine lorsque l'un des joueurs a gagné, la grille est pleine sans alignement, ou qu'un joueur abandonne.\n",
    "## Utilisation de l'environnement\n",
    "L'environnement du jeu du morpion est utilisé dans le cadre de l'apprentissage par renforcement pour entraîner des agents à jouer au morpion de manière autonome. Les agents peuvent apprendre à jouer au morpion en interagissant avec l'environnement, en apprenant à prédire les conséquences de leurs actions et à maximiser leurs chances de victoire.\n",
    "\n",
    "## Conclusion\n",
    "En conclusion, l'environnement du jeu du morpion est un cadre informatique simple mais efficace pour explorer les concepts de l'apprentissage par renforcement et de l'intelligence artificielle. Il offre une plateforme d'expérimentation où les agents peuvent apprendre à jouer à un jeu classique en utilisant des techniques d'apprentissage automatique, tout en permettant aux chercheurs et aux développeurs de tester et de comparer différentes stratégies et algorithmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titre: Découverte de l'environnement du jeu du Morpion\n",
    "### Introduction\n",
    "Le jeu du morpion, également connu sous le nom de Tic-Tac-Toe, est un jeu de société classique qui se joue sur une grille de 3x3 cases. Le jeu oppose deux joueurs, qui alternent pour placer leur marque (X ou O) dans une case vide de la grille. Le premier joueur à aligner trois de ses marques horizontalement, verticalement ou en diagonale remporte la partie.\n",
    "\n",
    "### Description de l'environnement\n",
    "L'environnement du jeu du morpion est une représentation informatique du jeu, permettant à des agents d'apprentissage ou à des joueurs humains de jouer au morpion de manière virtuelle. La grille de jeu est généralement représentée sous forme de tableau à deux dimensions, où chaque case peut être vide, contenant la marque du joueur X ou la marque du joueur O.\n",
    "\n",
    "### Caractéristiques de l'environnement\n",
    "- Grille de jeu : La grille de jeu est une matrice de 3x3 cases, où chaque case peut être vide, contenant la marque X ou la marque O.\n",
    "- Actions : Les actions disponibles pour chaque joueur consistent à sélectionner une case vide de la grille où placer sa marque.\n",
    "- Observations : L'état de l'environnement est représenté par la disposition actuelle des marques sur la grille.\n",
    "- Récompenses : La récompense est généralement attribuée à la fin de la partie, lorsque l'un des joueurs a aligné trois de ses marques ou que la grille est pleine sans alignement.\n",
    "- Terminalité : La partie se termine lorsque l'un des joueurs a gagné, la grille est pleine sans alignement, ou qu'un joueur abandonne.\n",
    "### Utilisation de l'environnement\n",
    "L'environnement du jeu du morpion est utilisé dans le cadre de l'apprentissage par renforcement pour entraîner des agents à jouer au morpion de manière autonome. Les agents peuvent apprendre à jouer au morpion en interagissant avec l'environnement, en apprenant à prédire les conséquences de leurs actions et à maximiser leurs chances de victoire.\n",
    "\n",
    "### Conclusion\n",
    "En conclusion, l'environnement du jeu du morpion est un cadre informatique simple mais efficace pour explorer les concepts de l'apprentissage par renforcement et de l'intelligence artificielle. Il offre une plateforme d'expérimentation où les agents peuvent apprendre à jouer à un jeu classique en utilisant des techniques d'apprentissage automatique, tout en permettant aux chercheurs et aux développeurs de tester et de comparer différentes stratégies et algorithmes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titre : Comprendre le fonctionnement de l'environnement du Double Q-learning avec un exemple pratique dans le jeu du Morpion\n",
    "\n",
    "### Introduction\n",
    "Le Double Q-learning est une extension de l'algorithme Q-learning classique, conçue pour surmonter les biais de surestimation des valeurs Q. Cet algorithme est particulièrement utile dans les environnements où les récompenses sont bruyantes ou sujettes à des variations importantes. Dans cet exposé, nous allons examiner le fonctionnement de l'environnement du Double Q-learning et illustrer son utilisation à travers un exemple pratique dans le jeu du Morpion.\n",
    "\n",
    "### Fonctionnement de l'environnement du Double Q-learning\n",
    "L'environnement du Double Q-learning utilise deux ensembles de valeurs Q distincts pour estimer la valeur d'une action dans un état donné. L'idée centrale est d'atténuer les biais de surestimation des valeurs Q en alternant entre ces deux ensembles lors de la mise à jour.\n",
    "\n",
    "- Initialisation des deux ensembles de valeurs Q : Au début, les deux ensembles de valeurs Q sont initialisés à zéro ou à des valeurs aléatoires.\n",
    "\n",
    "- Sélection de l'action : L'agent utilise l'un des ensembles de valeurs Q pour sélectionner l'action suivante. Cela peut être fait en choisissant simplement l'action avec la valeur Q maximale dans cet ensemble.\n",
    "\n",
    "- Estimation de la valeur de l'action : Une fois que l'action a été sélectionnée et exécutée dans l'environnement, l'agent utilise l'autre ensemble de valeurs Q pour estimer la valeur de cette action. Cette estimation est utilisée pour mettre à jour les valeurs Q.\n",
    "\n",
    "- Mise à jour des valeurs Q : Les valeurs Q sont mises à jour en utilisant une règle de mise à jour appropriée, telle que la règle de mise à jour du Q-learning. Cependant, au lieu de mettre à jour les deux ensembles de valeurs Q lors de chaque étape, un seul ensemble est mis à jour en fonction de l'estimation de la valeur de l'action faite par l'autre ensemble.\n",
    "\n",
    "Exemple pratique : Morpion\n",
    "Pour illustrer le fonctionnement de l'environnement du Double Q-learning, nous utiliserons le jeu du Morpion comme exemple pratique.\n",
    "\n",
    "### Environnement du Morpion : Nous créerons un environnement de jeu du Morpion où deux joueurs (X et O) s'affrontent sur une grille de 3x3 cases.\n",
    "\n",
    "- Double Q-learning Agent : Nous implémenterons un agent d'apprentissage par renforcement utilisant l'algorithme - - Double Q-learning pour apprendre à jouer au Morpion contre lui-même.\n",
    "\n",
    "- Entraînement de l'agent : L'agent sera entraîné à jouer au Morpion en interagissant avec l'environnement du jeu, en utilisant les deux ensembles de valeurs Q pour estimer les valeurs d'action et les mettre à jour.\n",
    "\n",
    "- Évaluation de l'agent : Nous évaluerons les performances de l'agent entraîné en le faisant jouer contre lui-même ou contre un joueur humain.\n",
    "\n",
    "## Conclusion\n",
    "En conclusion, l'environnement du Double Q-learning est une extension puissante de l'algorithme Q-learning, permettant de surmonter les biais de surestimation des valeurs Q. En utilisant deux ensembles de valeurs Q distincts, cet algorithme offre une approche robuste pour l'apprentissage par renforcement dans divers environnements, y compris les jeux comme le Morpion. Son utilisation pratique dans des exemples comme le jeu du Morpion illustre son efficacité et sa pertinence dans le domaine de l'intelligence artificielle et de l'apprentissage automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "         # Initialise l'environnement du jeu du Morpion\n",
    "        self.board = np.zeros((3, 3), dtype=int)  # Crée une grille de jeu vide de 3x3 cases\n",
    "        self.action_space = spaces.Discrete(9)  # Définit l'espace d'actions (0-8) pour les mouvements possibles\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(3, 3), dtype=np.int32) # Définit l'espace d'observation\n",
    "\n",
    "    def reset(self):\n",
    "        # Réinitialise l'environnement à un état initial\n",
    "        self.board = np.zeros((3, 3), dtype=int) # Réinitialise la grille de jeu\n",
    "        return self.board\n",
    "\n",
    "    def step(self, action):\n",
    "         # Exécute une action dans l'environnement et retourne les nouvelles observations, récompense, état de terminalité et informations supplémentaires\n",
    "        row = action // 3 # Calcule la ligne de la case choisie\n",
    "        col = action % 3 # Calcule la colonne de la case choisie\n",
    "\n",
    "        if self.board[row][col] != 0:\n",
    "            # Vérifie si la case est déjà occupée\n",
    "            return self.board, -10, True, {}  # Retourne l'état actuel, une récompense négative, True (jeu terminé) et des informations supplémentaires vides\n",
    "\n",
    "        self.board[row][col] = 1  # Place la marque du joueur sur la case choisie\n",
    "\n",
    "        if self.check_winner() == 1:\n",
    "            # Vérifie si le joueur actuel a gagné\n",
    "            return self.board, 10, True, {} # Retourne l'état actuel, une récompense positive, True (jeu terminé) et des informations supplémentaires vides\n",
    "\n",
    "        if self.is_board_full():\n",
    "            # Vérifie si la grille est pleine sans gagnant\n",
    "            return self.board, 0, True, {} # Retourne l'état actuel, une récompense nulle, True (jeu terminé) et des informations supplémentaires vides\n",
    "\n",
    "        available_actions = np.where(self.board == 0)  # Récupère les actions possibles (cases vides)\n",
    "        opponent_action = np.random.choice(len(available_actions[0]))  # Choix d'une action aléatoire pour l'adversaire\n",
    "        self.board[available_actions[0][opponent_action]][available_actions[1][opponent_action]] = -1 # Place la marque de l'adversaire\n",
    "\n",
    "        if self.check_winner() == -1:\n",
    "            # Vérifie si l'adversaire a gagné\n",
    "            return self.board, -10, True, {} # Retourne l'état actuel, une récompense négative, True (jeu terminé) et des informations supplémentaires vides\n",
    "\n",
    "        if self.is_board_full():\n",
    "            # Vérifie si la grille est pleine sans gagnant\n",
    "            return self.board, 0, True, {} # Retourne l'état actuel, une récompense nulle, True (jeu terminé) et des informations supplémentaires vides\n",
    "\n",
    "        return self.board, 0, False, {} # Retourne l'état actuel, une récompense nulle, False (jeu non terminé) et des informations supplémentaires vides\n",
    "\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Vérifie s'il y a un gagnant sur la grille\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != 0:\n",
    "                return self.board[i][0] # Retourne la marque du gagnant\n",
    "            if self.board[0][i] == self.board[1][i] == self.board[2][i] != 0:\n",
    "                return self.board[0][i]\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != 0:\n",
    "            return self.board[0][0]\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != 0:\n",
    "            return self.board[0][2]\n",
    "        return 0\n",
    "\n",
    "    def is_board_full(self):\n",
    "        return not (self.board == 0).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Q-learning agent\n",
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.1, gamma=0.99):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q1_table = {}\n",
    "        self.q2_table = {}\n",
    "\n",
    "    def get_action(self, state, available_actions):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(available_actions)\n",
    "        else:\n",
    "            if state not in self.q1_table:\n",
    "                self.q1_table[state] = np.zeros(9)\n",
    "            if state not in self.q2_table:\n",
    "                self.q2_table[state] = np.zeros(9)\n",
    "            q_values = self.q1_table[state] + self.q2_table[state]\n",
    "            return np.argmax(q_values[available_actions])\n",
    "\n",
    "    def update_q_tables(self, state, action, reward, next_state):\n",
    "        if state not in self.q1_table:\n",
    "            self.q1_table[state] = np.zeros(9)\n",
    "        if state not in self.q2_table:\n",
    "            self.q2_table[state] = np.zeros(9)\n",
    "        if np.random.rand() < 0.5:\n",
    "            max_next_action = np.argmax(self.q1_table[next_state])\n",
    "            self.q1_table[state][action] += self.alpha * (reward + self.gamma * self.q2_table[next_state][max_next_action] - self.q1_table[state][action])\n",
    "        else:\n",
    "            max_next_action = np.argmax(self.q2_table[next_state])\n",
    "            self.q2_table[state][action] += self.alpha * (reward + self.gamma * self.q1_table[next_state][max_next_action] - self.q2_table[state][action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "env = TicTacToeEnv()\n",
    "agent = DoubleQLearningAgent()\n",
    "\n",
    "for episode in range(10000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        available_actions = np.where(state == 0)[0]\n",
    "        action = agent.get_action(tuple(state.flatten()), available_actions)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.update_q_tables(tuple(state.flatten()), action, reward, tuple(next_state.flatten()))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Reward: 0\n",
      "Current state:\n",
      "[[ 1  0  0]\n",
      " [-1  0  0]\n",
      " [ 0  0  0]]\n",
      "Reward: 0\n",
      "Current state:\n",
      "[[ 1  1  0]\n",
      " [-1  0  0]\n",
      " [-1  0  0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[0;32m      7\u001b[0m available_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(state \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter action (0-8): \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# Playing against the agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    print(\"Current state:\")\n",
    "    print(state)\n",
    "    available_actions = np.where(state == 0)[0]\n",
    "    action = int(input(\"Enter action (0-8): \"))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(\"Reward:\", reward)\n",
    "    if done:\n",
    "        print(\"Game Over\")\n",
    "        print(\"Final state:\")\n",
    "        print(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
